deploy:
  phases:
    install:
      commands:
      - bash install_build.sh
    build:
      commands:
      # Export all env params specific to the deployment stuff
      - export ROOT_DIR=$(pwd)
      - export CLUSTER_NAME=${ADDF_PARAMETER_EKS_CLUSTER_NAME}
      - export CLUSTER_REGION=${AWS_DEFAULT_REGION}
      - export INSTALLATION_OPTION=${ADDF_PARAMETER_INSTALLATION_OPTION} #helm
      - export DEPLOYMENT_OPTION=${ADDF_PARAMETER_DEPLOYMENT_OPTION} #vanilla
      - export KUBEFLOW_RELEASE_VERSION=${ADDF_PARAMETER_KUBEFLOW_RELEASE_VERSION} #v1.6.1
      - export AWS_KUBEFLOW_BUILD=${ADDF_PARAMETER_AWS_KUBEFLOW_BUILD} #1.0.0
      - export AWS_RELEASE_VERSION=${KUBEFLOW_RELEASE_VERSION}-aws-b${AWS_KUBEFLOW_BUILD}
      ## Create and attach the policy to allow the KF admin role to create/delete roles
      - export KF_POLICY_NAME=addf-${ADDF_DEPLOYMENT_NAME}-${ADDF_MODULE_NAME}-${AWS_DEFAULT_REGION}-kf
      - python manage_admin_user.py create ${KF_POLICY_NAME} ${ADDF_PARAMETER_EKS_CLUSTER_MASTER_ROLE_ARN} 
      # Clone the AWS-on-kubeflow and change to the branch
      - git clone https://github.com/awslabs/kubeflow-manifests.git && cd kubeflow-manifests
      - git checkout $AWS_RELEASE_VERSION
      - git clone --branch ${KUBEFLOW_RELEASE_VERSION} https://github.com/kubeflow/manifests.git upstream
      - git status
      - python3.8 -m pip install -r tests/e2e/requirements.txt
      # Assuming EKS Role
      - cd $ROOT_DIR/kubeflow-manifests
      - eval $(aws sts assume-role --role-arn ${ADDF_PARAMETER_EKS_CLUSTER_MASTER_ROLE_ARN} --role-session-name aws-auth-ops | jq -r '.Credentials | "export AWS_ACCESS_KEY_ID=\(.AccessKeyId)\nexport AWS_SECRET_ACCESS_KEY=\(.SecretAccessKey)\nexport AWS_SESSION_TOKEN=\(.SessionToken)\n"')
      - aws eks update-kubeconfig --name ${CLUSTER_NAME}
      - kubectl get pods -n kube-system
      # Deploy Kubeflow here
      - make deploy-kubeflow INSTALLATION_OPTION=$INSTALLATION_OPTION DEPLOYMENT_OPTION=$DEPLOYMENT_OPTION
      # Make sure the IRSA is created with proper annotations, if not, DELETE any eksctl and redeploy to force it (supports redeployments)
      - cd $ROOT_DIR/
      - export DEP_MOD=${AWS_CODESEEDER_NAME}-${ADDF_DEPLOYMENT_NAME}-${ADDF_MODULE_NAME}-${AWS_DEFAULT_REGION}
      - export POLICY_NAME=${DEP_MOD}-policy
      - export SA_ROLE_NAME=${DEP_MOD}-sa-role
      - export SA_NAME=profiles-controller-service-account
      - export SA_ANNOTATION=$(kubectl get sa ${SA_NAME} -n kubeflow -o json | jq ".metadata.annotations" | grep "eks.amazonaws.com/role-arn")
      - >
        if [[ ! "$SA_ANNOTATION" ]]; then 
            aws iam get-policy --policy-arn arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${POLICY_NAME:0:60} || \
            aws iam create-policy --policy-name ${POLICY_NAME:0:60} --policy-document file://policies/iam-profile-sa-policy.json;
            eksctl delete iamserviceaccount --name ${SA_NAME} --namespace kubeflow --cluster ${ADDF_PARAMETER_EKS_CLUSTER_NAME} || true;
            eksctl create iamserviceaccount --name ${SA_NAME} --namespace kubeflow \
            --cluster ${ADDF_PARAMETER_EKS_CLUSTER_NAME} --role-name ${SA_ROLE_NAME:0:60} \
            --attach-policy-arn arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${POLICY_NAME:0:60} \
            --override-existing-serviceaccounts --approve;
            kubectl rollout restart deployment profiles-deployment -n kubeflow;
        fi
      - unset AWS_ACCESS_KEY_ID && unset AWS_SECRET_ACCESS_KEY && unset AWS_SESSION_TOKEN
destroy:
  phases:
    install:
      commands:
      - bash install_build.sh
    build:
      commands:
      # Reverse the order of deploy...delete the Servuce Account
      - eval $(aws sts assume-role --role-arn ${ADDF_PARAMETER_EKS_CLUSTER_MASTER_ROLE_ARN} --role-session-name aws-auth-ops | jq -r '.Credentials | "export AWS_ACCESS_KEY_ID=\(.AccessKeyId)\nexport AWS_SECRET_ACCESS_KEY=\(.SecretAccessKey)\nexport AWS_SESSION_TOKEN=\(.SessionToken)\n"')
      - export DEP_MOD=${AWS_CODESEEDER_NAME}-${ADDF_DEPLOYMENT_NAME}-${ADDF_MODULE_NAME}-${AWS_DEFAULT_REGION}
      - export POLICY_NAME=${DEP_MOD}-policy
      - export SA_ROLE_NAME=${DEP_MOD}-sa-role
      - export SA_NAME=profiles-controller-service-account
      - eksctl delete iamserviceaccount --name ${SA_NAME} --namespace kubeflow --cluster ${ADDF_PARAMETER_EKS_CLUSTER_NAME}
      # Give the Stack time to resolve and release the policy
      - sleep 10;
      - aws iam delete-policy  --policy-arn arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${POLICY_NAME:0:60} || true

      # Export all env params specific to the deployment stuff
      - export ROOT_DIR=$(pwd)
      - export CLUSTER_NAME=${ADDF_PARAMETER_EKS_CLUSTER_NAME}
      - export CLUSTER_REGION=${AWS_DEFAULT_REGION}
      - export INSTALLATION_OPTION=${ADDF_PARAMETER_INSTALLATION_OPTION} #helm
      - export DEPLOYMENT_OPTION=${ADDF_PARAMETER_DEPLOYMENT_OPTION} #vanilla
      - export KUBEFLOW_RELEASE_VERSION=${ADDF_PARAMETER_KUBEFLOW_RELEASE_VERSION} #v1.6.1
      - export AWS_KUBEFLOW_BUILD=${ADDF_PARAMETER_AWS_KUBEFLOW_BUILD} #1.0.0
      - export AWS_RELEASE_VERSION=${KUBEFLOW_RELEASE_VERSION}-aws-b${AWS_KUBEFLOW_BUILD}
      - export KF_POLICY_NAME=addf-${ADDF_DEPLOYMENT_NAME}-${ADDF_MODULE_NAME}-${AWS_DEFAULT_REGION}-kf
      - git clone https://github.com/awslabs/kubeflow-manifests.git && cd kubeflow-manifests
      - git checkout $AWS_RELEASE_VERSION
      - git clone --branch ${KUBEFLOW_RELEASE_VERSION} https://github.com/kubeflow/manifests.git upstream
      - git status
      - python3.8 -m pip install -r tests/e2e/requirements.txt

      - aws eks update-kubeconfig --name ${CLUSTER_NAME}
      # Destroy it here
      - kubectl get profiles -o json |  jq -r '.items[].metadata.name' >> profiles.out
      - for name in $(cat profiles.out); do kubectl patch profile $name --type json -p '{"metadata":{"finalizers":null}}' --type=merge; done  || true
      - make delete-kubeflow INSTALLATION_OPTION=$INSTALLATION_OPTION DEPLOYMENT_OPTION=$DEPLOYMENT_OPTION
      - unset AWS_ACCESS_KEY_ID && unset AWS_SECRET_ACCESS_KEY && unset AWS_SESSION_TOKEN
      # Unattach the policy from  KF admin role and delete the policy
      - cd $ROOT_DIR
      - python manage_admin_user.py delete ${KF_POLICY_NAME} ${ADDF_PARAMETER_EKS_CLUSTER_MASTER_ROLE_ARN}

build_type: BUILD_GENERAL1_SMALL